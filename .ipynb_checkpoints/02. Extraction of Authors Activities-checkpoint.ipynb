{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c55415f6",
   "metadata": {},
   "source": [
    "# Download all comments and posts made by authors of a subreddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83006e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw \n",
    "import pandas as pd\n",
    "import tqdm\n",
    "import math\n",
    "import json\n",
    "import requests\n",
    "import itertools\n",
    "import numpy as np\n",
    "import time\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457a88e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Way too slow. Didn't have to use this wrapper. Better use Pushshift.io directly to extract authors comments and posts\n",
    "# # Authentication \n",
    "# reddit = praw.Reddit(client_id='RLHw-HpxYJqUR7UJLrBQ1Q',\n",
    "#                      client_secret='ytB8OnsUKW68-k6ydmmv-RyX3mq_Mw',\n",
    "#                      user_agent='loa_crawler',\n",
    "#                      username='',\n",
    "#                      password='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d80f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function to make request to Pushshift\n",
    "\"\"\"\n",
    "def make_request(uri, max_retries = 5):\n",
    "    def fire_away(uri):\n",
    "        response = requests.get(uri)\n",
    "        assert response.status_code == 200\n",
    "        return json.loads(response.content)\n",
    "    current_tries = 1\n",
    "    while current_tries < max_retries:\n",
    "        try:\n",
    "            time.sleep(1)\n",
    "            response = fire_away(uri)\n",
    "            return response\n",
    "        except:\n",
    "            time.sleep(1)\n",
    "            current_tries += 1\n",
    "    return fire_away(uri)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Load and process posts and comments from a subreddit\n",
    "\"\"\"\n",
    "def get_processed_df(file_path):\n",
    "    \n",
    "    df_subreddit = pd.read_csv(f\"{file_path}.csv\")\n",
    "    df_subreddit = df_subreddit[df_subreddit['author']!=\"[deleted]\"] \n",
    "    df_subreddit = df_subreddit[df_subreddit['text']!=\"[deleted]\"] \n",
    "    df_subreddit = df_subreddit[df_subreddit['text']!=\"[removed]\"]\n",
    "    df_subreddit[\"text\"].fillna(\" \", inplace = True)\n",
    "    df_subreddit = df_subreddit.drop_duplicates()\n",
    "    \n",
    "    return df_subreddit\n",
    "\n",
    "\"\"\"\n",
    "Get list of unique authors posting at a given subreddit\n",
    "\"\"\"\n",
    "def get_unique_authors(df_subreddit):\n",
    "    \n",
    "    # Only take users who posted more than once\n",
    "    repeating = df_subreddit[df_subreddit.duplicated(['author'], keep = False)] \n",
    "    # Get rid of deleted users\n",
    "    repeating = repeating[repeating.author != 'None'] \n",
    "    u_authors = list(repeating.author.unique()) \n",
    "    \n",
    "    print(\"Number of unique authors :\", len(u_authors))\n",
    "    return u_authors\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Download all comments made anywhere by a given list of authors\n",
    "\"\"\"\n",
    "def download_author_comments(u_authors, file_path):\n",
    "    authors_list = []\n",
    "\n",
    "    for u in tqdm.tqdm(range(len(u_authors))): \n",
    "\n",
    "        uri = f\"https://api.pushshift.io/reddit/comment/search/?author={u_authors[u]}&filter=created_utc,author,subreddit,permalink,body\"\n",
    "        try:\n",
    "            result = make_request(uri, \n",
    "                          max_retries= 2)\n",
    "            authors_list.extend(result['data'])\n",
    "        except Exception as e:\n",
    "            print(\"ERROR :\", e)\n",
    "            pass\n",
    "\n",
    "    authors_df = pd.DataFrame(authors_list)\n",
    "    authors_df['isodate'] = authors_df['created_utc'].apply(lambda x: f\"{datetime.fromtimestamp(x):%F %T}\")\n",
    "    authors_df.drop(columns = ['created_utc'], inplace = True)\n",
    "    \n",
    "    authors_df.to_csv(f'{file_path}_comments.csv', index = False)\n",
    "    \n",
    "    return authors_df\n",
    "    \n",
    "    \n",
    "\"\"\"\n",
    "Download all posts made anywhere by a given list of authors\n",
    "\"\"\"   \n",
    "def download_author_posts(u_authors, file_path):\n",
    "    authors_posts = []\n",
    "\n",
    "    for u in tqdm.tqdm(range(len(u_authors))):\n",
    "\n",
    "        uri = f\"https://api.pushshift.io/reddit/submission/search/?author={u_authors[u]}&filter=created_utc,author,subreddit,permalink,title,selftext\"\n",
    "        try:\n",
    "            result = make_request(uri, \n",
    "                          max_retries= 2)\n",
    "            authors_posts.extend(result['data'])\n",
    "        except Exception as e:\n",
    "            print(\"ERROR :\", e)\n",
    "            pass\n",
    "\n",
    "    authors_df = pd.DataFrame(authors_posts)\n",
    "    authors_df['isodate'] = authors_df['created_utc'].apply(lambda x: f\"{datetime.fromtimestamp(x):%F %T}\")\n",
    "    authors_df.drop(columns = ['created_utc'], inplace = True)\n",
    "    \n",
    "    authors_df.to_csv(f'{file_path}_posts.csv', index = False)\n",
    "    \n",
    "    return authors_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8f7197",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Get all posts from a particular subreddit\n",
    "Set subreddit here\n",
    "\"\"\"\n",
    "\n",
    "subreddit = 'Nietzsche'\n",
    "file_path = f\"{subreddit}/{subreddit}\"\n",
    "\n",
    "df_subreddit = get_processed_df(file_path)\n",
    "u_authors = get_unique_authors(df_subreddit)\n",
    "\n",
    "print(df_subreddit.shape)\n",
    "df_subreddit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16617bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_posts = download_author_posts(u_authors, file_path)\n",
    "print(authors_posts.shape)\n",
    "authors_posts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3270af",
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_comments = download_author_comments(u_authors, file_path)\n",
    "print(authors_comments.shape)\n",
    "authors_comments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dcbfaa0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
